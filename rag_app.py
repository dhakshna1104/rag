# -*- coding: utf-8 -*-
"""RAG Final.ipynp

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18dx4xGPsfPtAqhOvxWgXx-LozDDB_8SV
"""

!pip install numpy pandas scikit-learn requests beautifulsoup4 nltk faiss-cpu tiktoken

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

#!/usr/bin/env python3
"""
Quick fix for NLTK data requirements
Run this if you encounter NLTK errors
"""

import nltk

def download_nltk_data():
    """Download required NLTK data with proper fallbacks"""
    print("üì• Downloading NLTK data...")

    # List of resources to download with fallback options
    resources = [
        ('punkt_tab', 'punkt'),  # New version first, old version fallback
        ('stopwords',),
        ('wordnet',),
        ('omw-1.4',)  # Multilingual wordnet data
    ]

    success_count = 0

    for resource_options in resources:
        downloaded = False
        for resource in resource_options:
            try:
                print(f"Downloading {resource}...")
                nltk.download(resource, quiet=False)
                print(f"‚úÖ Successfully downloaded {resource}")
                downloaded = True
                success_count += 1
                break
            except Exception as e:
                print(f"‚ö†Ô∏è  Failed to download {resource}: {e}")
                continue

        if not downloaded:
            print(f"‚ùå Could not download any of: {', '.join(resource_options)}")

    print(f"\nüìä Downloaded {success_count}/{len(resources)} resource groups")

    if success_count >= 2:  # At least punkt and stopwords
        print("‚úÖ Minimum required data downloaded. The RAG system should work now.")
    else:
        print("‚ö†Ô∏è  Some downloads failed. The system will use fallback methods.")

    # Test the downloads
    print("\nüß™ Testing NLTK functionality...")

    try:
        from nltk.tokenize import sent_tokenize, word_tokenize
        test_text = "Hello world. This is a test."
        sentences = sent_tokenize(test_text)
        words = word_tokenize(test_text)
        print(f"‚úÖ Sentence tokenization works: {len(sentences)} sentences")
        print(f"‚úÖ Word tokenization works: {len(words)} words")
    except Exception as e:
        print(f"‚ùå Tokenization test failed: {e}")

    try:
        from nltk.corpus import stopwords
        stops = stopwords.words('english')
        print(f"‚úÖ Stopwords loaded: {len(stops)} words")
    except Exception as e:
        print(f"‚ùå Stopwords test failed: {e}")

    try:
        from nltk.stem import WordNetLemmatizer
        lemmatizer = WordNetLemmatizer()
        result = lemmatizer.lemmatize("running", "v")
        print(f"‚úÖ Lemmatization works: 'running' -> '{result}'")
    except Exception as e:
        print(f"‚ùå Lemmatization test failed: {e}")

if __name__ == "__main__":
    print("üîß NLTK Data Fix Script")
    print("=" * 30)
    download_nltk_data()
    print("\nüèÅ Setup complete! Try running the RAG system again.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile rag_app.py
# #!/usr/bin/env python3
# 
# """
# Complete Standalone RAG (Retrieval Augmented Generation) System
# A single-file implementation for processing websites and answering questions
# 
# Usage:
#     python complete_rag_system.py
# 
# Requirements:
#     pip install numpy pandas scikit-learn requests beautifulsoup4 nltk faiss-cpu tiktoken
# """
# 
# import os
# import re
# import json
# import pickle
# import logging
# import numpy as np
# import pandas as pd
# from typing import List, Dict, Tuple, Optional, Any
# from dataclasses import dataclass, asdict
# from datetime import datetime
# import hashlib
# from pathlib import Path
# import time
# 
# # Core libraries
# import requests
# from bs4 import BeautifulSoup
# from urllib.parse import urljoin, urlparse
# 
# # Vector operations and similarity
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.metrics.pairwise import cosine_similarity
# from sklearn.decomposition import TruncatedSVD
# 
# # Text processing
# try:
#     import nltk
#     from nltk.tokenize import sent_tokenize, word_tokenize
#     from nltk.corpus import stopwords
#     from nltk.stem import WordNetLemmatizer
# 
#     def download_nltk_data():
#         downloads = [
#             ('punkt_tab', 'punkt'),
#             ('stopwords',),
#             ('wordnet',),
#             ('omw-1.4',)
#         ]
#         for download_options in downloads:
#             for resource in download_options:
#                 try:
#                     nltk.download(resource, quiet=True)
#                     break
#                 except:
#                     continue
#     try:
#         download_nltk_data()
#     except:
#         print("Warning: Some NLTK downloads failed. Using basic fallbacks.")
#     NLTK_AVAILABLE = True
# except ImportError:
#     print("NLTK not available. Using basic text processing.")
#     NLTK_AVAILABLE = False
# 
# try:
#     import faiss
#     FAISS_AVAILABLE = True
# except ImportError:
#     print("FAISS not available. Using basic similarity search.")
#     FAISS_AVAILABLE = False
# 
# try:
#     import tiktoken
#     TIKTOKEN_AVAILABLE = True
# except ImportError:
#     print("tiktoken not available. Using basic tokenization.")
#     TIKTOKEN_AVAILABLE = False
# 
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# logger = logging.getLogger(__name__)
# 
# @dataclass
# class Document:
#     id: str
#     title: str
#     content: str
#     url: str
#     source: str
#     timestamp: datetime
#     word_count: int
#     embeddings: Optional[np.ndarray] = None
#     metadata: Dict[str, Any] = None
# 
#     def __post_init__(self):
#         if self.metadata is None:
#             self.metadata = {}
# 
# class WebContentExtractor:
#     def __init__(self, request_delay: float = 1.0):
#         self.session = requests.Session()
#         self.session.headers.update({
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
#         })
#         self.request_delay = request_delay
# 
#     def extract_content(self, url: str) -> Optional[Document]:
#         try:
#             logger.info(f"Extracting content from: {url}")
#             response = self.session.get(url, timeout=30)
#             response.raise_for_status()
#             soup = BeautifulSoup(response.content, 'html.parser')
#             for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
#                 element.decompose()
#             title = soup.find('title')
#             title = title.get_text().strip() if title else urlparse(url).netloc
#             content_selectors = [
#                 'main', 'article', '.content', '.post', '.entry',
#                 '[role="main"]', '.main-content', '#content'
#             ]
#             content = ""
#             for selector in content_selectors:
#                 elements = soup.select(selector)
#                 if elements:
#                     content = elements[0].get_text(separator=' ', strip=True)
#                     break
#             if not content:
#                 paragraphs = soup.find_all(['p', 'div', 'section'])
#                 content = ' '.join([p.get_text(strip=True) for p in paragraphs])
#             content = self._clean_text(content)
#             if len(content) < 100:
#                 logger.warning(f"Content too short for {url}")
#                 return None
#             doc_id = hashlib.md5(url.encode()).hexdigest()
#             return Document(
#                 id=doc_id,
#                 title=title,
#                 content=content,
#                 url=url,
#                 source=urlparse(url).netloc,
#                 timestamp=datetime.now(),
#                 word_count=len(content.split()),
#                 metadata={
#                     'content_length': len(content),
#                     'extraction_method': 'web_scraping'
#                 }
#             )
#         except Exception as e:
#             logger.error(f"Error extracting content from {url}: {str(e)}")
#             return None
#     def _clean_text(self, text: str) -> str:
#         text = re.sub(r'\s+', ' ', text)
#         text = re.sub(r'[^\w\s.,;:!?-]', ' ', text)
#         text = re.sub(r'[.,;:!?]{2,}', '.', text)
#         return text.strip()
# 
# class TextChunker:
#     def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
#         self.chunk_size = chunk_size
#         self.chunk_overlap = chunk_overlap
#         if TIKTOKEN_AVAILABLE:
#             try:
#                 self.tokenizer = tiktoken.get_encoding("cl100k_base")
#             except:
#                 self.tokenizer = None
#         else:
#             self.tokenizer = None
#     def _count_tokens(self, text: str) -> int:
#         if self.tokenizer:
#             return len(self.tokenizer.encode(text))
#         else:
#             return len(text.split()) * 1.3
#     def chunk_by_sentences(self, text: str, max_chunk_size: int = None) -> List[str]:
#         if max_chunk_size is None:
#             max_chunk_size = self.chunk_size
#         if NLTK_AVAILABLE:
#             try:
#                 sentences = sent_tokenize(text)
#             except LookupError:
#                 logger.warning("NLTK sentence tokenizer not available, using regex fallback")
#                 sentences = re.split(r'[.!?]+(?:\s+|$)', text)
#                 sentences = [s.strip() for s in sentences if s.strip()]
#         else:
#             sentences = re.split(r'[.!?]+(?:\s+|$)', text)
#             sentences = [s.strip() for s in sentences if s.strip()]
#         chunks = []
#         current_chunk = []
#         current_size = 0
#         for sentence in sentences:
#             sentence_tokens = self._count_tokens(sentence)
#             if current_size + sentence_tokens > max_chunk_size and current_chunk:
#                 chunks.append(' '.join(current_chunk))
#                 overlap_sentences = current_chunk[-2:] if len(current_chunk) > 2 else current_chunk
#                 current_chunk = overlap_sentences + [sentence]
#                 current_size = sum(self._count_tokens(s) for s in current_chunk)
#             else:
#                 current_chunk.append(sentence)
#                 current_size += sentence_tokens
#         if current_chunk:
#             chunks.append(' '.join(current_chunk))
#         return chunks
#     def chunk_document(self, document: Document) -> List[Document]:
#         chunks = self.chunk_by_sentences(document.content)
#         chunked_docs = []
#         for i, chunk in enumerate(chunks):
#             chunk_id = f"{document.id}_chunk_{i}"
#             chunked_doc = Document(
#                 id=chunk_id,
#                 title=f"{document.title} (Chunk {i+1})",
#                 content=chunk,
#                 url=document.url,
#                 source=document.source,
#                 timestamp=document.timestamp,
#                 word_count=len(chunk.split()),
#                 metadata={
#                     **document.metadata,
#                     'parent_doc_id': document.id,
#                     'chunk_index': i,
#                     'total_chunks': len(chunks)
#                 }
#             )
#             chunked_docs.append(chunked_doc)
#         return chunked_docs
# 
# class VectorDatabase:
#     def __init__(self, embedding_dim: int, use_faiss: bool = True):  #¬†*** CHANGED: require embedding_dim
#         self.embedding_dim = embedding_dim
#         self.use_faiss = use_faiss and FAISS_AVAILABLE
#         self.documents: Dict[str, Document] = {}
#         self.embeddings_matrix: Optional[np.ndarray] = None
#         self.doc_ids: List[str] = []
#         if self.use_faiss:
#             self.index = faiss.IndexFlatIP(embedding_dim)
#             logger.info(f"Initialized FAISS index with dim={embedding_dim}")
#         else:
#             logger.info("Using basic similarity search (FAISS not available)")
#     def add_documents(self, documents: List[Document], embeddings: np.ndarray):
#         if len(documents) != embeddings.shape[0]:
#             raise ValueError("Number of documents must match number of embeddings")
#         if embeddings.shape[1] != self.embedding_dim:
#             raise ValueError(f"Embedding dimension mismatch: got {embeddings.shape[1]}, expected {self.embedding_dim}")
#         embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)
#         for i, doc in enumerate(documents):
#             self.documents[doc.id] = doc
#             self.doc_ids.append(doc.id)
#             doc.embeddings = embeddings[i]
#         if self.use_faiss:
#             self.index.add(embeddings.astype(np.float32))
#         else:
#             if self.embeddings_matrix is None:
#                 self.embeddings_matrix = embeddings
#             else:
#                 self.embeddings_matrix = np.vstack([self.embeddings_matrix, embeddings])
#         logger.info(f"Added {len(documents)} documents to vector database")
#     # ...rest unchanged
# 
#     def search(self, query_embedding: np.ndarray, k: int = 5, threshold: float = 0.0) -> List[Tuple[Document, float]]:
#         if len(self.documents) == 0:
#             return []
#         query_embedding = query_embedding / np.linalg.norm(query_embedding)
#         query_embedding = query_embedding.reshape(1, -1).astype(np.float32)
#         if self.use_faiss:
#             scores, indices = self.index.search(query_embedding, min(k, len(self.documents)))
#             results = []
#             for score, idx in zip(scores[0], indices[0]):
#                 if score > threshold and idx < len(self.doc_ids):
#                     doc_id = self.doc_ids[idx]
#                     doc = self.documents[doc_id]
#                     results.append((doc, float(score)))
#         else:
#             similarities = cosine_similarity(query_embedding, self.embeddings_matrix)[0]
#             top_indices = np.argsort(similarities)[::-1][:k]
#             results = []
#             for idx in top_indices:
#                 if similarities[idx] > threshold:
#                     doc_id = self.doc_ids[idx]
#                     doc = self.documents[doc_id]
#                     results.append((doc, float(similarities[idx])))
#         return results
# 
#     # ...rest unchanged
# 
#     def save(self, filepath: str):
#         # ...unchanged except for pickling
#         data = {
#             'documents': {doc_id: asdict(doc) for doc_id, doc in self.documents.items()},
#             'doc_ids': self.doc_ids,
#             'embedding_dim': self.embedding_dim,
#             'use_faiss': self.use_faiss
#         }
#         for doc_data in data['documents'].values():
#             doc_data['timestamp'] = doc_data['timestamp'].isoformat()
#             if doc_data['embeddings'] is not None:
#                 doc_data['embeddings'] = doc_data['embeddings'].tolist()
#         if self.use_faiss:
#             index_path = filepath.replace('.pkl', '_index.faiss')
#             faiss.write_index(self.index, index_path)
#         else:
#             embeddings_path = filepath.replace('.pkl', '_embeddings.npy')
#             if self.embeddings_matrix is not None:
#                 np.save(embeddings_path, self.embeddings_matrix)
#         with open(filepath, 'wb') as f:
#             pickle.dump(data, f)
#         logger.info(f"Saved vector database to {filepath}")
# 
#     def load(self, filepath: str):
#         with open(filepath, 'rb') as f:
#             data = pickle.load(f)
#         self.documents = {}
#         for doc_id, doc_data in data['documents'].items():
#             doc_data['timestamp'] = datetime.fromisoformat(doc_data['timestamp'])
#             if doc_data['embeddings'] is not None:
#                 doc_data['embeddings'] = np.array(doc_data['embeddings'])
#             self.documents[doc_id] = Document(**doc_data)
#         self.doc_ids = data['doc_ids']
#         self.embedding_dim = data['embedding_dim']
#         self.use_faiss = data.get('use_faiss', True) and FAISS_AVAILABLE
#         if self.use_faiss:
#             index_path = filepath.replace('.pkl', '_index.faiss')
#             if os.path.exists(index_path):
#                 self.index = faiss.read_index(index_path)
#             else:
#                 self.index = faiss.IndexFlatIP(self.embedding_dim)
#         else:
#             embeddings_path = filepath.replace('.pkl', '_embeddings.npy')
#             if os.path.exists(embeddings_path):
#                 self.embeddings_matrix = np.load(embeddings_path)
#         logger.info(f"Loaded vector database from {filepath}")
# 
# class TextEmbedder:
#     def __init__(self, embedding_dim: int = 384, max_features: int = 10000):
#         self.embedding_dim = embedding_dim
#         self.max_features = max_features
#         self.vectorizer = TfidfVectorizer(
#             max_features=max_features,
#             stop_words='english',
#             ngram_range=(1, 2),
#             max_df=0.95,
#             min_df=2
#         )
#         self.svd = None  # created as needed
#         self.is_fitted = False
#         if NLTK_AVAILABLE:
#             try:
#                 self.lemmatizer = WordNetLemmatizer()
#                 self.stop_words = set(stopwords.words('english'))
#             except LookupError:
#                 logger.warning("NLTK data not fully available, using basic preprocessing")
#                 self.lemmatizer = None
#                 self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])
#         else:
#             self.lemmatizer = None
#             self.stop_words = set(['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'])
#     def _preprocess_text(self, text: str) -> str:
#         text = text.lower()
#         if NLTK_AVAILABLE and self.lemmatizer:
#             try:
#                 tokens = word_tokenize(text)
#                 tokens = [
#                     self.lemmatizer.lemmatize(token)
#                     for token in tokens
#                     if token.isalpha() and token not in self.stop_words
#                 ]
#             except LookupError:
#                 logger.warning("NLTK tokenizer not available, using regex fallback")
#                 tokens = re.findall(r'\b[a-zA-Z]+\b', text)
#                 tokens = [token for token in tokens if token not in self.stop_words]
#         else:
#             tokens = re.findall(r'\b[a-zA-Z]+\b', text)
#             tokens = [token for token in tokens if token not in self.stop_words]
#         return ' '.join(tokens)
#     def fit_transform(self, texts: List[str]) -> np.ndarray:
#         preprocessed_texts = [self._preprocess_text(text) for text in texts]
#         tfidf_matrix = self.vectorizer.fit_transform(preprocessed_texts)
#         n_samples, n_features = tfidf_matrix.shape
#         adjusted_dim = min(self.embedding_dim, n_samples - 1, n_features)
#         self.svd = TruncatedSVD(n_components=adjusted_dim, random_state=42)
#         embeddings = self.svd.fit_transform(tfidf_matrix.toarray())
#         self.is_fitted = True
#         self.embedding_dim = adjusted_dim  # update the actual embedded dimension!
#         logger.info(f"Fitted embedder on {len(texts)} documents with embedding_dim {adjusted_dim}")
#         return embeddings.astype(np.float32)
#     def transform(self, texts: List[str]) -> np.ndarray:
#         if not self.is_fitted:
#             raise ValueError("Embedder not fitted. Call fit_transform first.")
#         preprocessed_texts = [self._preprocess_text(text) for text in texts]
#         tfidf_matrix = self.vectorizer.transform(preprocessed_texts)
#         embeddings = self.svd.transform(tfidf_matrix.toarray())
#         return embeddings.astype(np.float32)
# 
# class RetrieverRAG:
#     def __init__(self, vector_db: VectorDatabase, embedder: TextEmbedder):
#         self.vector_db = vector_db
#         self.embedder = embedder
#     def retrieve(self, query: str, k: int = 5, threshold: float = 0.1) -> List[Tuple[Document, float]]:
#         query_embedding = self.embedder.transform([query])[0]
#         results = self.vector_db.search(query_embedding, k=k, threshold=threshold)
#         logger.info(f"Retrieved {len(results)} documents for query: {query[:50]}...")
#         return results
#     def generate_response(self, query: str, retrieved_docs: List[Tuple[Document, float]], max_context_length: int = 2000) -> str:
#         if not retrieved_docs:
#             return "I couldn't find relevant information to answer your question."
#         context_parts = []
#         current_length = 0
#         for doc, score in retrieved_docs:
#             doc_text = f"Source: {doc.title}\nContent: {doc.content[:500]}...\n"
#             if current_length + len(doc_text) > max_context_length:
#                 break
#             context_parts.append(doc_text)
#             current_length += len(doc_text)
#         context = "\n".join(context_parts)
#         response = f"""Based on the retrieved documents, here's what I found relevant to your query "{query}":
# 
# {context}
# 
# Summary: The retrieved documents contain information about {', '.join([doc.title for doc, _ in retrieved_docs[:3]])}.
# Most relevant match has a similarity score of {retrieved_docs[0][1]:.3f}."""
#         return response
# 
# class RAGPipeline:
#     def __init__(self, storage_dir: str = "rag_storage"):
#         self.storage_dir = Path(storage_dir)
#         self.storage_dir.mkdir(exist_ok=True)
#         self.extractor = WebContentExtractor()
#         self.chunker = TextChunker()
#         self.embedder = TextEmbedder()
#         self.vector_db = None  # do not pre-instantiate!
#         self.retriever = None
# 
#     def build_knowledge_base(self, urls: List[str]) -> Dict[str, Any]:
#         logger.info("Starting knowledge base construction...")
#         documents = []
#         failed_urls = []
#         for url in urls:
#             try:
#                 doc = self.extractor.extract_content(url)
#                 if doc:
#                     documents.append(doc)
#                 else:
#                     failed_urls.append(url)
#                 time.sleep(self.extractor.request_delay)
#             except Exception as e:
#                 logger.error(f"Failed to process {url}: {e}")
#                 failed_urls.append(url)
#         if not documents:
#             raise ValueError("No documents could be extracted from provided URLs")
#         all_chunks = []
#         for doc in documents:
#             chunks = self.chunker.chunk_document(doc)
#             all_chunks.extend(chunks)
#         logger.info(f"Created {len(all_chunks)} chunks from {len(documents)} documents")
#         texts = [chunk.content for chunk in all_chunks]
#         embeddings = self.embedder.fit_transform(texts)
#         actual_dim = embeddings.shape[1]
#         self.vector_db = VectorDatabase(embedding_dim=actual_dim)
#         self.vector_db.add_documents(all_chunks, embeddings)
#         self.retriever = RetrieverRAG(self.vector_db, self.embedder)
#         self.save()
#         return {
#             'total_documents': len(documents),
#             'total_chunks': len(all_chunks),
#             'failed_urls': failed_urls,
#             'embedding_dimension': self.embedder.embedding_dim
#         }
#     def query(self, question: str, k: int = 5) -> Dict[str, Any]:
#         if self.retriever is None:
#             try:
#                 self.load()
#             except:
#                 raise ValueError("No knowledge base found. Please build one first using build_knowledge_base().")
#         retrieved_docs = self.retriever.retrieve(question, k=k)
#         response = self.retriever.generate_response(question, retrieved_docs)
#         return {
#             'question': question,
#             'response': response,
#             'retrieved_documents': [
#                 {
#                     'title': doc.title,
#                     'content': doc.content[:200] + "...",
#                     'score': score,
#                     'source': doc.source,
#                     'url': doc.url
#                 }
#                 for doc, score in retrieved_docs
#             ],
#             'num_retrieved': len(retrieved_docs)
#         }
#     def save(self):
#         db_path = self.storage_dir / "vector_db.pkl"
#         if self.vector_db:
#             self.vector_db.save(str(db_path))
#         embedder_path = self.storage_dir / "embedder.pkl"
#         with open(embedder_path, 'wb') as f:
#             pickle.dump({
#                 'vectorizer': self.embedder.vectorizer,
#                 'svd': self.embedder.svd,
#                 'embedding_dim': self.embedder.embedding_dim,
#                 'max_features': self.embedder.max_features,
#                 'is_fitted': self.embedder.is_fitted
#             }, f)
#         logger.info("Saved RAG pipeline")
#     def load(self):
#         db_path = self.storage_dir / "vector_db.pkl"
#         if db_path.exists():
#             if not self.vector_db:
#                 with open(self.storage_dir / "embedder.pkl", 'rb') as f:
#                     embedder_data = pickle.load(f)
#                     actual_dim = embedder_data['embedding_dim']
#                 self.vector_db = VectorDatabase(embedding_dim=actual_dim)
#             self.vector_db.load(str(db_path))
#         embedder_path = self.storage_dir / "embedder.pkl"
#         if embedder_path.exists():
#             with open(embedder_path, 'rb') as f:
#                 embedder_data = pickle.load(f)
#                 self.embedder.vectorizer = embedder_data['vectorizer']
#                 self.embedder.svd = embedder_data['svd']
#                 self.embedder.embedding_dim = embedder_data['embedding_dim']
#                 self.embedder.max_features = embedder_data['max_features']
#                 self.embedder.is_fitted = embedder_data['is_fitted']
#         if self.vector_db and self.vector_db.documents:
#             self.retriever = RetrieverRAG(self.vector_db, self.embedder)
#         logger.info("Loaded RAG pipeline")
# 
# def main():
#     print("üöÄ RAG System - Complete Implementation")
#     print("=" * 50)
#     print("üì¶ Checking dependencies...")
#     missing_deps = []
#     if not FAISS_AVAILABLE:
#         missing_deps.append("faiss-cpu")
#     if not NLTK_AVAILABLE:
#         missing_deps.append("nltk")
#     if not TIKTOKEN_AVAILABLE:
#         missing_deps.append("tiktoken")
#     if missing_deps:
#         print(f"‚ö†Ô∏è  Missing optional dependencies: {', '.join(missing_deps)}")
#         print("   Install with: pip install " + " ".join(missing_deps))
#         print("   System will use fallback implementations.")
#     else:
#         print("‚úÖ All dependencies available!")
#     print("\n" + "=" * 50)
#     rag = RAGPipeline()
#     urls = [
#         "https://huyenchip.com/2024/07/25/genai-platform.html",
#         "https://lilianweng.github.io/posts/2024-07-07-hallucination",
#         "https://jina.ai/news/what-is-colbert-and-late-interaction-and-why-they-matter-in-search/",
#     ]
#     try:
#         print("üî® Building knowledge base...")
#         stats = rag.build_knowledge_base(urls)
#         print("\nüìä Knowledge Base Statistics:")
#         for key, value in stats.items():
#             print(f"   ‚Ä¢ {key}: {value}")
#         queries = [
#             "What is RAG and how does it work?",
#             "What are the main components of a generative AI platform?",
#             "How can we prevent hallucinations in LLMs?",
#             "What is ColBERT and late interaction?",
#         ]
#         print("\nüîç Testing Queries:")
#         print("=" * 50)
#         for i, query in enumerate(queries, 1):
#             print(f"\n{i}. Query: {query}")
#             print("-" * 40)
#             try:
#                 start_time = time.time()
#                 result = rag.query(query, k=3)
#                 query_time = time.time() - start_time
#                 print(f"‚è±Ô∏è  Query time: {query_time:.2f} seconds")
#                 print(f"üìÑ Retrieved {result['num_retrieved']} documents")
#                 if result['retrieved_documents']:
#                     print("üéØ Top documents:")
#                     for j, doc in enumerate(result['retrieved_documents'][:2]):
#                         print(f"   {j+1}. {doc['title'][:50]}... (score: {doc['score']:.3f})")
#                 response_preview = result['response'][:300].replace('\n', ' ')
#                 print(f"üí¨ Response: {response_preview}...\n")
#             except Exception as e:
#                 print(f"‚ùå Error: {e}")
#         print("‚úÖ RAG system demonstration completed!")
#     except Exception as e:
#         print(f"üí• Error: {e}")
#         logger.error(f"Main execution error: {e}", exc_info=True)
# 
# if __name__ == "__main__":
#     main()

!pip install fastapi uvicorn

from fastapi import FastAPI, Header, HTTPException, Depends
from typing import Optional, List
from pydantic import BaseModel
import os

# Import your existing RAGPipeline class from the module where it's defined
# For demonstration, assuming it's in rag_system.py
from rag_app import RAGPipeline

app = FastAPI(title="RAG API")

API_TOKEN = os.getenv("NGROK_AUTH_TOKEN", "31ghHKsxoox0X6RIQuCHWjttLy3_83KXBuvCK5fJW1uEyQpbf")

def verify_token(x_api_key: Optional[str] = Header(None)):
    if x_api_key != API_TOKEN:
        raise HTTPException(status_code=401, detail="Unauthorized")

class IndexRequest(BaseModel):
    urls: List[str]

class ChatRequest(BaseModel):
    question: str
    k: Optional[int] = 5

# Initialize shared RAG pipeline object
rag = RAGPipeline()

@app.post("/api/v1/index", dependencies=[Depends(verify_token)])
async def index_data(request: IndexRequest):
    try:
        stats = rag.build_knowledge_base(request.urls)
        return {
            "status": "Knowledge base built successfully",
            "total_documents": stats["total_documents"],
            "total_chunks": stats["total_chunks"],
            "failed_urls": stats["failed_urls"],
            "embedding_dimension": stats["embedding_dimension"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error building knowledge base: {str(e)}")

@app.post("/api/v1/chat", dependencies=[Depends(verify_token)])
async def chat(request: ChatRequest):
    try:
        if rag.retriever is None:
            rag.load()
        result = rag.query(request.question, k=request.k)
        return {
            "question": request.question,
            "response": result["response"],
            "retrieved_documents": result["retrieved_documents"],
            "num_retrieved": result["num_retrieved"]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing query: {str(e)}")

!pip install streamlit

import streamlit as st
from rag_app import RAGPipeline

def main_streamlit_app():
    st.set_page_config(layout="wide")
    st.title("RAG QA System")

    # Load RAG pipeline and knowledge base at startup, no URL input needed
    @st.cache_resource
    def get_rag_pipeline():
        return RAGPipeline(storage_dir="streamlit_rag_storage")
    rag = get_rag_pipeline()
    rag.load()
    kb_loaded = (rag.vector_db is not None and len(rag.vector_db.documents) > 0)

    if not kb_loaded:
        st.warning("Knowledge base is not loaded. Please ensure it exists in storage.")
        st.stop()

    st.header("Ask a Question")
    question = st.text_input("Enter your question here")

    k = st.slider("Number of documents to retrieve (k)", min_value=1, max_value=10, value=3)
    threshold = st.slider("Similarity Threshold", min_value=0.0, max_value=1.0, value=0.0, step=0.05)

    if st.button("Get Answer"):
        if question.strip() == "":
            st.error("Please enter a question to query.")
        else:
            with st.spinner("Retrieving answer..."):
                result = rag.query(question, k=k, threshold=threshold)
                st.subheader("Answer")
                st.write(result["response"])
                st.subheader(f"Retrieved Documents ({result['num_retrieved']})")
                if result["retrieved_documents"]:
                    for i, doc in enumerate(result["retrieved_documents"], 1):
                        st.markdown(f"**{i}. {doc['title']}** (score: {doc['score']:.3f})")
                        st.text_area(f"Document {i} Content", value=doc["content"], height=200, key=f"doc_content_{i}")
                        st.write(f"[Source]({doc['url']})")
                else:
                    st.info("No documents retrieved for this query.")



# Commented out IPython magic to ensure Python compatibility.
# %%writefile minimalistic_rag_app.py
# import streamlit as st
# from rag_app import RAGPipeline
# 
# rag = RAGPipeline(storage_dir="rag_storage")
# 
# st.title("Minimalistic RAG System UI")
# st.header("Ask a Question")
# 
# question = st.text_input("Enter your question here", key="question_input")
# 
# try:
#     rag.load()
#     kb_loaded = (rag.vector_db is not None and len(rag.vector_db.documents) > 0)
#     if kb_loaded:
#         st.info(f"Knowledge base loaded with {len(rag.vector_db.documents)} indexed chunks.")
#         st.sidebar.write("Knowledge Base Loaded ‚úÖ")
#         st.sidebar.write(f"Indexed Chunks: {len(rag.vector_db.documents)}")
#         st.sidebar.write(f"Embedding Dim: {getattr(rag.embedder, 'actual_embedding_dim', 'N/A')}")
#     else:
#         st.warning("No knowledge base found. Please contact the administrator.")
#         st.sidebar.write("Knowledge Base Not Loaded ‚ùå")
# except Exception as e:
#     st.error(f"Error loading knowledge base state: {e}")
#     kb_loaded = False
#     st.sidebar.write("Knowledge Base Not Loaded ‚ùå")
# 
# k = st.slider("Number of documents to retrieve (k)", min_value=1, max_value=10, value=3, key="k_slider", disabled=not kb_loaded)
# threshold = st.slider("Similarity Threshold", min_value=0.0, max_value=1.0, value=0.0, step=0.05, key="threshold_slider", disabled=not kb_loaded)
# 
# if st.button("Get Answer", key="get_answer_button", disabled=not kb_loaded):
#     if not question.strip():
#         st.error("Please enter a question to query.")
#     else:
#         with st.spinner("Retrieving answer..."):
#             try:
#                 result = rag.query(question, k=k)
#                 st.subheader("Answer")
#                 st.write(result["response"])
# 
#                 st.subheader(f"Retrieved Documents ({result['num_retrieved']})")
#                 if result["retrieved_documents"]:
#                     for i, doc in enumerate(result["retrieved_documents"], 1):
#                         st.markdown(f"**{i}. {doc['title']}** (score: {doc['score']:.3f})")
#                         st.text_area(f"Document {i} Content", value=doc["content"], height=200, key=f"doc_content_{i}")
#                         st.write(f"[Source]({doc['url']})")
#                 else:
#                     st.info("No documents retrieved for this query.")
#             except Exception as e:
#                 st.error(f"Error retrieving answer: {e}")
#

import urllib
print("Password/Enpoint IP for localtunnel is:",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip("\n"))

!streamlit run minimalistic_rag_app.py & npx localtunnel --port 8501
